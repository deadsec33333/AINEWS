{"id":16543,"date":"2024-11-22T15:47:04","date_gmt":"2024-11-22T15:47:04","guid":{"rendered":"https:\/\/www.artificialintelligence-news.com\/?p=16543"},"modified":"2024-11-22T15:47:05","modified_gmt":"2024-11-22T15:47:05","slug":"openai-enhances-ai-safety-new-red-teaming-methods","status":"publish","type":"post","link":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/","title":{"rendered":"OpenAI enhances AI safety with new red teaming methods"},"content":{"rendered":"\n<p>A critical part of OpenAI\u2019s safeguarding process is &#8220;red teaming&#8221; \u2014 a structured methodology using both human and AI participants to explore potential risks and vulnerabilities in new systems.<\/p>\n\n\n\n<p>Historically, OpenAI has engaged in red teaming efforts predominantly through manual testing, which involves individuals probing for weaknesses. This was notably employed during the testing of their DALL\u00b7E 2 image generation model in early 2022, where external experts were invited to identify potential risks. Since then, OpenAI has expanded and refined its methodologies, incorporating automated and mixed approaches for a more comprehensive risk assessment.<\/p>\n\n\n\n<p>&#8220;We are optimistic that we can use more powerful AI to scale the discovery of model mistakes,&#8221; OpenAI stated. This optimism is rooted in the idea that automated processes can help evaluate models and train them to be safer by recognising patterns and errors on a larger scale.<\/p>\n\n\n\n<p>In their latest push for advancement, OpenAI is sharing two important documents on red teaming \u2014 a white paper detailing external engagement strategies and a research study introducing a novel method for automated red teaming. These contributions aim to strengthen the process and outcomes of red teaming, ultimately leading to safer and more responsible AI implementations.<\/p>\n\n\n\n<p>As AI continues to evolve, understanding user experiences and identifying risks such as abuse and misuse are crucial for researchers and developers. Red teaming provides a proactive method for evaluating these risks, especially when supplemented by insights from a range of independent external experts. This approach not only helps establish benchmarks but also facilitates the enhancement of safety evaluations over time.<\/p>\n\n\n\n<h3 class=\"wp-block-heading\">The human touch<\/h3>\n\n\n\n<p>OpenAI has shared four fundamental steps in their white paper, <a href=\"https:\/\/cdn.openai.com\/papers\/openais-approach-to-external-red-teaming.pdf\">&#8220;OpenAI\u2019s Approach to External Red Teaming for AI Models and Systems,&#8221;<\/a> to design effective red teaming campaigns:<\/p>\n\n\n\n<ol class=\"wp-block-list\">\n<li><strong>Composition of red teams:<\/strong> The selection of team members is based on the objectives of the campaign. This often involves individuals with diverse perspectives, such as expertise in natural sciences, cybersecurity, and regional politics, ensuring assessments cover the necessary breadth.<\/li>\n<\/ol>\n\n\n\n<ol start=\"2\" class=\"wp-block-list\">\n<li><strong>Access to model versions:<\/strong> Clarifying which versions of a model red teamers will access can influence the outcomes. Early-stage models may reveal inherent risks, while more developed versions can help identify gaps in planned safety mitigations.<\/li>\n<\/ol>\n\n\n\n<ol start=\"3\" class=\"wp-block-list\">\n<li><strong>Guidance and documentation:<\/strong> Effective interactions during campaigns rely on clear instructions, suitable interfaces, and structured documentation. This involves describing the models, existing safeguards, testing interfaces, and guidelines for recording results.<\/li>\n<\/ol>\n\n\n\n<ol start=\"4\" class=\"wp-block-list\">\n<li><strong>Data synthesis and evaluation:<\/strong> Post-campaign, the data is assessed to determine if examples align with existing policies or require new behavioural modifications. The assessed data then informs repeatable evaluations for future updates.<\/li>\n<\/ol>\n\n\n\n<p>A recent application of this methodology involved preparing the OpenAI <a href=\"https:\/\/openai.com\/index\/learning-to-reason-with-llms\/\">o1 family<\/a> of models for public use\u2014testing their resistance to potential misuse and evaluating their application across various fields such as real-world attack planning, natural sciences, and AI research.<\/p>\n\n\n\n<h3 class=\"wp-block-heading\">Automated red teaming<\/h3>\n\n\n\n<p>Automated red teaming seeks to identify instances where AI may fail, particularly regarding safety-related issues. This method excels at scale, generating numerous examples of potential errors quickly. However, traditional automated approaches have struggled with producing diverse, successful attack strategies.<\/p>\n\n\n\n<p>OpenAI&#8217;s research introduces <a href=\"https:\/\/cdn.openai.com\/papers\/diverse-and-effective-red-teaming.pdf\">&#8220;Diverse And Effective Red Teaming With Auto-Generated Rewards And Multi-Step Reinforcement Learning,&#8221;<\/a> a method which encourages greater diversity in attack strategies while maintaining effectiveness.<\/p>\n\n\n\n<p>This method involves using AI to generate different scenarios, such as illicit advice, and training red teaming models to evaluate these scenarios critically. The process rewards diversity and efficacy, promoting more varied and comprehensive safety evaluations.<\/p>\n\n\n\n<p>Despite its benefits, red teaming does have limitations. It captures risks at a specific point in time, which may evolve as AI models develop. Additionally, the red teaming process can inadvertently create information hazards, potentially alerting malicious actors to vulnerabilities not yet widely known. Managing these risks requires stringent protocols and responsible disclosures.<\/p>\n\n\n\n<p>While red teaming continues to be pivotal in risk discovery and evaluation, OpenAI acknowledges the necessity of incorporating broader public perspectives on AI&#8217;s ideal behaviours and policies to ensure the technology aligns with societal values and expectations.<\/p>\n\n\n\n<p><strong>See also: <\/strong><a href=\"https:\/\/www.artificialintelligence-news.com\/news\/eu-introduces-draft-regulatory-guidance-for-ai-models\/\"><strong>EU introduces draft regulatory guidance for AI models<\/strong><\/a><\/p>\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><a href=\"https:\/\/www.ai-expo.net\/\"><img loading=\"lazy\" decoding=\"async\" width=\"728\" height=\"90\" src=\"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2022\/04\/ai-expo-world-728x-90-01.png\" alt=\"\" class=\"wp-image-11874\" style=\"width:959px;height:auto\" srcset=\"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2022\/04\/ai-expo-world-728x-90-01.png 728w, https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2022\/04\/ai-expo-world-728x-90-01-300x37.png 300w, https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2022\/04\/ai-expo-world-728x-90-01-380x47.png 380w, https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2022\/04\/ai-expo-world-728x-90-01-350x43.png 350w, https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2022\/04\/ai-expo-world-728x-90-01-100x12.png 100w, https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2022\/04\/ai-expo-world-728x-90-01-60x7.png 60w\" sizes=\"auto, (max-width: 728px) 100vw, 728px\" \/><\/a><\/figure>\n\n\n\n<p><strong>Want to learn more about AI and big data from industry leaders?<\/strong> Check out<a href=\"https:\/\/www.ai-expo.net\/\"> AI &amp; Big Data Expo<\/a> taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including <a href=\"https:\/\/intelligentautomation-conference.com\/northamerica\/\">Intelligent Automation Conference<\/a>, <a href=\"https:\/\/www.blockchain-expo.com\/\">BlockX<\/a>,<a href=\"https:\/\/digitaltransformation-week.com\/\"> Digital Transformation Week<\/a>, and <a href=\"https:\/\/www.cybersecuritycloudexpo.com\/\">Cyber Security &amp; Cloud Expo<\/a>.<\/p>\n\n\n\n<p>Explore other upcoming enterprise technology events and webinars powered by TechForge <a href=\"https:\/\/techforge.pub\/events\/\">here<\/a>.<\/p>\n","protected":false},"excerpt":{"rendered":"<p>A critical part of OpenAI\u2019s safeguarding process is &#8220;red teaming&#8221; \u2014 a structured methodology using both human and AI participants to explore potential risks and vulnerabilities in new systems. Historically, OpenAI has engaged in red teaming efforts predominantly through manual testing, which involves individuals probing for weaknesses. This was notably employed during the testing of<a class=\"excerpt-read-more\" href=\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/\" title=\"ReadOpenAI enhances AI safety with new red teaming methods\">&#8230; Read more &raquo;<\/a><\/p>\n","protected":false},"author":1570,"featured_media":16545,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":{"_acf_changed":false,"om_disable_all_campaigns":false,"_monsterinsights_skip_tracking":false,"_monsterinsights_sitenote_active":false,"_monsterinsights_sitenote_note":"","_monsterinsights_sitenote_category":0,"_uf_show_specific_survey":0,"_uf_disable_surveys":false,"footnotes":""},"categories":[1,1900,1906,303],"tags":[186,192,366,442,224,3142,1024,543],"ppma_author":[2401],"class_list":["post-16543","post","type-post","status-publish","format-standard","has-post-thumbnail","hentry","category-artificial-intelligence","category-ai-companies","category-development","category-ai-ethics-society","tag-ai","tag-artificial-intelligence","tag-development","tag-ethics","tag-openai","tag-red-teaming","tag-safety","tag-society"],"acf":[],"yoast_head":"<!-- This site is optimized with the Yoast SEO plugin v24.1 - https:\/\/yoast.com\/wordpress\/plugins\/seo\/ -->\n<title>OpenAI enhances AI safety with new red teaming methods<\/title>\n<meta name=\"description\" content=\"OpenAI historically engaged in AI red teaming efforts through manual testing, which involves individuals probing for weaknesses.\" \/>\n<meta name=\"robots\" content=\"index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\" \/>\n<link rel=\"canonical\" href=\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/\" \/>\n<meta property=\"og:locale\" content=\"en_GB\" \/>\n<meta property=\"og:type\" content=\"article\" \/>\n<meta property=\"og:title\" content=\"OpenAI enhances AI safety with new red teaming methods\" \/>\n<meta property=\"og:description\" content=\"OpenAI historically engaged in AI red teaming efforts through manual testing, which involves individuals probing for weaknesses.\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/\" \/>\n<meta property=\"og:site_name\" content=\"AI News\" \/>\n<meta property=\"article:publisher\" content=\"https:\/\/www.facebook.com\/AITechNews\/\" \/>\n<meta property=\"article:published_time\" content=\"2024-11-22T15:47:04+00:00\" \/>\n<meta property=\"article:modified_time\" content=\"2024-11-22T15:47:05+00:00\" \/>\n<meta property=\"og:image\" content=\"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2024\/11\/openai-ai-red-teaming-methods-techniques-safety-development-ethics-society-artificial-intelligence.png\" \/>\n\t<meta property=\"og:image:width\" content=\"1430\" \/>\n\t<meta property=\"og:image:height\" content=\"1024\" \/>\n\t<meta property=\"og:image:type\" content=\"image\/png\" \/>\n<meta name=\"author\" content=\"Ryan Daws\" \/>\n<meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n<meta name=\"twitter:creator\" content=\"@Gadget_Ry\" \/>\n<meta name=\"twitter:site\" content=\"@ai_technews\" \/>\n<meta name=\"twitter:label1\" content=\"Written by\" \/>\n\t<meta name=\"twitter:data1\" content=\"Ryan Daws\" \/>\n\t<meta name=\"twitter:label2\" content=\"Estimated reading time\" \/>\n\t<meta name=\"twitter:data2\" content=\"4 minutes\" \/>\n<script type=\"application\/ld+json\" class=\"yoast-schema-graph\">{\"@context\":\"https:\/\/schema.org\",\"@graph\":[{\"@type\":\"Article\",\"@id\":\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#article\",\"isPartOf\":{\"@id\":\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/\"},\"author\":{\"name\":\"Ryan Daws\",\"@id\":\"https:\/\/www.ianj52.sg-host.com\/#\/schema\/person\/e4d76ff18520c27fd0713eff05f814ed\"},\"headline\":\"OpenAI enhances AI safety with new red teaming methods\",\"datePublished\":\"2024-11-22T15:47:04+00:00\",\"dateModified\":\"2024-11-22T15:47:05+00:00\",\"mainEntityOfPage\":{\"@id\":\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/\"},\"wordCount\":740,\"commentCount\":0,\"publisher\":{\"@id\":\"https:\/\/www.ianj52.sg-host.com\/#organization\"},\"image\":{\"@id\":\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#primaryimage\"},\"thumbnailUrl\":\"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2024\/11\/openai-ai-red-teaming-methods-techniques-safety-development-ethics-society-artificial-intelligence.png\",\"keywords\":[\"ai\",\"artificial intelligence\",\"development\",\"ethics\",\"openai\",\"red teaming\",\"safety\",\"Society\"],\"articleSection\":[\"Artificial Intelligence\",\"Companies\",\"Development\",\"Ethics &amp; Society\"],\"inLanguage\":\"en-GB\",\"potentialAction\":[{\"@type\":\"CommentAction\",\"name\":\"Comment\",\"target\":[\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#respond\"]}]},{\"@type\":\"WebPage\",\"@id\":\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/\",\"url\":\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/\",\"name\":\"OpenAI enhances AI safety with new red teaming methods\",\"isPartOf\":{\"@id\":\"https:\/\/www.ianj52.sg-host.com\/#website\"},\"primaryImageOfPage\":{\"@id\":\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#primaryimage\"},\"image\":{\"@id\":\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#primaryimage\"},\"thumbnailUrl\":\"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2024\/11\/openai-ai-red-teaming-methods-techniques-safety-development-ethics-society-artificial-intelligence.png\",\"datePublished\":\"2024-11-22T15:47:04+00:00\",\"dateModified\":\"2024-11-22T15:47:05+00:00\",\"description\":\"OpenAI historically engaged in AI red teaming efforts through manual testing, which involves individuals probing for weaknesses.\",\"breadcrumb\":{\"@id\":\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#breadcrumb\"},\"inLanguage\":\"en-GB\",\"potentialAction\":[{\"@type\":\"ReadAction\",\"target\":[\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/\"]}]},{\"@type\":\"ImageObject\",\"inLanguage\":\"en-GB\",\"@id\":\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#primaryimage\",\"url\":\"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2024\/11\/openai-ai-red-teaming-methods-techniques-safety-development-ethics-society-artificial-intelligence.png\",\"contentUrl\":\"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2024\/11\/openai-ai-red-teaming-methods-techniques-safety-development-ethics-society-artificial-intelligence.png\",\"width\":1430,\"height\":1024,\"caption\":\"A team of people in red illustrating new AI red teaming methods created by OpenAI to improve the safety of new artificial intelligence models.\"},{\"@type\":\"BreadcrumbList\",\"@id\":\"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#breadcrumb\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Home\",\"item\":\"https:\/\/www.artificialintelligence-news.com\/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"OpenAI enhances AI safety with new red teaming methods\"}]},{\"@type\":\"WebSite\",\"@id\":\"https:\/\/www.ianj52.sg-host.com\/#website\",\"url\":\"https:\/\/www.ianj52.sg-host.com\/\",\"name\":\"AI News\",\"description\":\"Artificial Intelligence News\",\"publisher\":{\"@id\":\"https:\/\/www.ianj52.sg-host.com\/#organization\"},\"potentialAction\":[{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https:\/\/www.ianj52.sg-host.com\/?s={search_term_string}\"},\"query-input\":{\"@type\":\"PropertyValueSpecification\",\"valueRequired\":true,\"valueName\":\"search_term_string\"}}],\"inLanguage\":\"en-GB\"},{\"@type\":\"Organization\",\"@id\":\"https:\/\/www.ianj52.sg-host.com\/#organization\",\"name\":\"AI News\",\"url\":\"https:\/\/www.ianj52.sg-host.com\/\",\"logo\":{\"@type\":\"ImageObject\",\"inLanguage\":\"en-GB\",\"@id\":\"https:\/\/www.ianj52.sg-host.com\/#\/schema\/logo\/image\/\",\"url\":\"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2020\/03\/ai-newsv4-2-svg.png\",\"contentUrl\":\"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2020\/03\/ai-newsv4-2-svg.png\",\"width\":400,\"height\":78,\"caption\":\"AI News\"},\"image\":{\"@id\":\"https:\/\/www.ianj52.sg-host.com\/#\/schema\/logo\/image\/\"},\"sameAs\":[\"https:\/\/www.facebook.com\/AITechNews\/\",\"https:\/\/x.com\/ai_technews\",\"https:\/\/www.linkedin.com\/groups\/1906826\/\"]},{\"@type\":\"Person\",\"@id\":\"https:\/\/www.ianj52.sg-host.com\/#\/schema\/person\/e4d76ff18520c27fd0713eff05f814ed\",\"name\":\"Ryan Daws\",\"image\":{\"@type\":\"ImageObject\",\"inLanguage\":\"en-GB\",\"@id\":\"https:\/\/www.ianj52.sg-host.com\/#\/schema\/person\/image\/0a89168c3b9fc190f9bdbce571d2fa5f\",\"url\":\"https:\/\/secure.gravatar.com\/avatar\/b8c5d238e1fddd55d8a0064f1a534ba5?s=96&d=mm&r=g\",\"contentUrl\":\"https:\/\/secure.gravatar.com\/avatar\/b8c5d238e1fddd55d8a0064f1a534ba5?s=96&d=mm&r=g\",\"caption\":\"Ryan Daws\"},\"description\":\"Ryan Daws is a senior editor at TechForge Media with over a decade of experience in crafting compelling narratives and making complex topics accessible. His articles and interviews with industry leaders have earned him recognition as a key influencer by organisations like Onalytica. Under his leadership, publications have been praised by analyst firms such as Forrester for their excellence and performance. Connect with him on X (@gadget_ry), Bluesky (@gadgetry.bsky.social), and\/or Mastodon (@gadgetry@techhub.social)\",\"sameAs\":[\"https:\/\/twitter.com\/gadget_ry\",\"https:\/\/x.com\/Gadget_Ry\"],\"url\":\"https:\/\/www.artificialintelligence-news.com\/news\/author\/ryan\/\"}]}<\/script>\n<!-- \/ Yoast SEO plugin. -->","yoast_head_json":{"title":"OpenAI enhances AI safety with new red teaming methods","description":"OpenAI historically engaged in AI red teaming efforts through manual testing, which involves individuals probing for weaknesses.","robots":{"index":"index","follow":"follow","max-snippet":"max-snippet:-1","max-image-preview":"max-image-preview:large","max-video-preview":"max-video-preview:-1"},"canonical":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/","og_locale":"en_GB","og_type":"article","og_title":"OpenAI enhances AI safety with new red teaming methods","og_description":"OpenAI historically engaged in AI red teaming efforts through manual testing, which involves individuals probing for weaknesses.","og_url":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/","og_site_name":"AI News","article_publisher":"https:\/\/www.facebook.com\/AITechNews\/","article_published_time":"2024-11-22T15:47:04+00:00","article_modified_time":"2024-11-22T15:47:05+00:00","og_image":[{"width":1430,"height":1024,"url":"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2024\/11\/openai-ai-red-teaming-methods-techniques-safety-development-ethics-society-artificial-intelligence.png","type":"image\/png"}],"author":"Ryan Daws","twitter_card":"summary_large_image","twitter_creator":"@Gadget_Ry","twitter_site":"@ai_technews","twitter_misc":{"Written by":"Ryan Daws","Estimated reading time":"4 minutes"},"schema":{"@context":"https:\/\/schema.org","@graph":[{"@type":"Article","@id":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#article","isPartOf":{"@id":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/"},"author":{"name":"Ryan Daws","@id":"https:\/\/www.ianj52.sg-host.com\/#\/schema\/person\/e4d76ff18520c27fd0713eff05f814ed"},"headline":"OpenAI enhances AI safety with new red teaming methods","datePublished":"2024-11-22T15:47:04+00:00","dateModified":"2024-11-22T15:47:05+00:00","mainEntityOfPage":{"@id":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/"},"wordCount":740,"commentCount":0,"publisher":{"@id":"https:\/\/www.ianj52.sg-host.com\/#organization"},"image":{"@id":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#primaryimage"},"thumbnailUrl":"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2024\/11\/openai-ai-red-teaming-methods-techniques-safety-development-ethics-society-artificial-intelligence.png","keywords":["ai","artificial intelligence","development","ethics","openai","red teaming","safety","Society"],"articleSection":["Artificial Intelligence","Companies","Development","Ethics &amp; Society"],"inLanguage":"en-GB","potentialAction":[{"@type":"CommentAction","name":"Comment","target":["https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#respond"]}]},{"@type":"WebPage","@id":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/","url":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/","name":"OpenAI enhances AI safety with new red teaming methods","isPartOf":{"@id":"https:\/\/www.ianj52.sg-host.com\/#website"},"primaryImageOfPage":{"@id":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#primaryimage"},"image":{"@id":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#primaryimage"},"thumbnailUrl":"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2024\/11\/openai-ai-red-teaming-methods-techniques-safety-development-ethics-society-artificial-intelligence.png","datePublished":"2024-11-22T15:47:04+00:00","dateModified":"2024-11-22T15:47:05+00:00","description":"OpenAI historically engaged in AI red teaming efforts through manual testing, which involves individuals probing for weaknesses.","breadcrumb":{"@id":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#breadcrumb"},"inLanguage":"en-GB","potentialAction":[{"@type":"ReadAction","target":["https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/"]}]},{"@type":"ImageObject","inLanguage":"en-GB","@id":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#primaryimage","url":"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2024\/11\/openai-ai-red-teaming-methods-techniques-safety-development-ethics-society-artificial-intelligence.png","contentUrl":"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2024\/11\/openai-ai-red-teaming-methods-techniques-safety-development-ethics-society-artificial-intelligence.png","width":1430,"height":1024,"caption":"A team of people in red illustrating new AI red teaming methods created by OpenAI to improve the safety of new artificial intelligence models."},{"@type":"BreadcrumbList","@id":"https:\/\/www.artificialintelligence-news.com\/news\/openai-enhances-ai-safety-new-red-teaming-methods\/#breadcrumb","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https:\/\/www.artificialintelligence-news.com\/"},{"@type":"ListItem","position":2,"name":"OpenAI enhances AI safety with new red teaming methods"}]},{"@type":"WebSite","@id":"https:\/\/www.ianj52.sg-host.com\/#website","url":"https:\/\/www.ianj52.sg-host.com\/","name":"AI News","description":"Artificial Intelligence News","publisher":{"@id":"https:\/\/www.ianj52.sg-host.com\/#organization"},"potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https:\/\/www.ianj52.sg-host.com\/?s={search_term_string}"},"query-input":{"@type":"PropertyValueSpecification","valueRequired":true,"valueName":"search_term_string"}}],"inLanguage":"en-GB"},{"@type":"Organization","@id":"https:\/\/www.ianj52.sg-host.com\/#organization","name":"AI News","url":"https:\/\/www.ianj52.sg-host.com\/","logo":{"@type":"ImageObject","inLanguage":"en-GB","@id":"https:\/\/www.ianj52.sg-host.com\/#\/schema\/logo\/image\/","url":"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2020\/03\/ai-newsv4-2-svg.png","contentUrl":"https:\/\/www.artificialintelligence-news.com\/wp-content\/uploads\/2020\/03\/ai-newsv4-2-svg.png","width":400,"height":78,"caption":"AI News"},"image":{"@id":"https:\/\/www.ianj52.sg-host.com\/#\/schema\/logo\/image\/"},"sameAs":["https:\/\/www.facebook.com\/AITechNews\/","https:\/\/x.com\/ai_technews","https:\/\/www.linkedin.com\/groups\/1906826\/"]},{"@type":"Person","@id":"https:\/\/www.ianj52.sg-host.com\/#\/schema\/person\/e4d76ff18520c27fd0713eff05f814ed","name":"Ryan Daws","image":{"@type":"ImageObject","inLanguage":"en-GB","@id":"https:\/\/www.ianj52.sg-host.com\/#\/schema\/person\/image\/0a89168c3b9fc190f9bdbce571d2fa5f","url":"https:\/\/secure.gravatar.com\/avatar\/b8c5d238e1fddd55d8a0064f1a534ba5?s=96&d=mm&r=g","contentUrl":"https:\/\/secure.gravatar.com\/avatar\/b8c5d238e1fddd55d8a0064f1a534ba5?s=96&d=mm&r=g","caption":"Ryan Daws"},"description":"Ryan Daws is a senior editor at TechForge Media with over a decade of experience in crafting compelling narratives and making complex topics accessible. His articles and interviews with industry leaders have earned him recognition as a key influencer by organisations like Onalytica. Under his leadership, publications have been praised by analyst firms such as Forrester for their excellence and performance. Connect with him on X (@gadget_ry), Bluesky (@gadgetry.bsky.social), and\/or Mastodon (@gadgetry@techhub.social)","sameAs":["https:\/\/twitter.com\/gadget_ry","https:\/\/x.com\/Gadget_Ry"],"url":"https:\/\/www.artificialintelligence-news.com\/news\/author\/ryan\/"}]}},"authors":[{"term_id":2401,"user_id":1570,"is_guest":0,"slug":"ryan","display_name":"Ryan Daws","avatar_url":{"url":"https:\/\/secure.gravatar.com\/avatar\/?s=96&d=mm&r=g","url2x":"https:\/\/secure.gravatar.com\/avatar\/?s=96&d=mm&r=g2x"},"user_url":"https:\/\/twitter.com\/gadget_ry","last_name":"Daws","first_name":"Ryan","job_title":"Senior Editor","description":"Ryan Daws is a senior editor at TechForge Media with over a decade of experience in crafting compelling narratives and making complex topics accessible. His articles and interviews with industry leaders have earned him recognition as a key influencer by organisations like Onalytica. Under his leadership, publications have been praised by analyst firms such as Forrester for their excellence and performance. Connect with him on X (@gadget_ry), Bluesky (@gadgetry.bsky.social), and\/or Mastodon (@gadgetry@techhub.social)"}],"_links":{"self":[{"href":"https:\/\/www.artificialintelligence-news.com\/wp-json\/wp\/v2\/posts\/16543","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/www.artificialintelligence-news.com\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/www.artificialintelligence-news.com\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/www.artificialintelligence-news.com\/wp-json\/wp\/v2\/users\/1570"}],"replies":[{"embeddable":true,"href":"https:\/\/www.artificialintelligence-news.com\/wp-json\/wp\/v2\/comments?post=16543"}],"version-history":[{"count":0,"href":"https:\/\/www.artificialintelligence-news.com\/wp-json\/wp\/v2\/posts\/16543\/revisions"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/www.artificialintelligence-news.com\/wp-json\/wp\/v2\/media\/16545"}],"wp:attachment":[{"href":"https:\/\/www.artificialintelligence-news.com\/wp-json\/wp\/v2\/media?parent=16543"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/www.artificialintelligence-news.com\/wp-json\/wp\/v2\/categories?post=16543"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/www.artificialintelligence-news.com\/wp-json\/wp\/v2\/tags?post=16543"},{"taxonomy":"author","embeddable":true,"href":"https:\/\/www.artificialintelligence-news.com\/wp-json\/wp\/v2\/ppma_author?post=16543"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}